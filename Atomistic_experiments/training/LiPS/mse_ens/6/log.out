/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231009_094643-3bqq1w2g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 6
wandb: â­ï¸ View project at https://wandb.ai/bananenpampe/LiPS-mse-ens
wandb: ğŸš€ View run at https://wandb.ai/bananenpampe/LiPS-mse-ens/runs/3bqq1w2g
seed 6
/home/kellner/BPNN_packages/H2O/fit_LiPS/mse_ensemble/6/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=3)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=15)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=16)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceRespone()
  )
  (loss_fn): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 194 K 
1 | loss_fn            | EnergyForceLoss        | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 3     
--------------------------------------------------------------
194 K     Trainable params
3         Non-trainable params
194 K     Total params
0.780     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00168: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00204: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00225: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00246: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00267: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00288: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00309: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00330: reducing learning rate of group 0 to 3.9063e-05.
Epoch 00351: reducing learning rate of group 0 to 1.9531e-05.
Epoch 00372: reducing learning rate of group 0 to 9.7656e-06.
Epoch 00393: reducing learning rate of group 0 to 4.8828e-06.
Epoch 00414: reducing learning rate of group 0 to 2.4414e-06.
Epoch 00435: reducing learning rate of group 0 to 1.2207e-06.
Epoch 00456: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=500` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Test metric             DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    test_energy_%rmse     0.00010100450739423218
     test_energy_mae        0.09436337858695154
     test_energy_mse        0.03221772759315164
    test_energy_rmse        0.17949297365956038
    test_forces_%rmse       0.12472896985040392
     test_forces_mae       0.056001027289528785
     test_forces_mse       0.010131563476017964
    test_forces_rmse        0.10065566787825693
        test_loss          0.042349291069169605
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   test_energy_%rmse â–
wandb:     test_energy_mae â–
wandb:     test_energy_mse â–
wandb:    test_energy_rmse â–
wandb:   test_forces_%rmse â–
wandb:     test_forces_mae â–
wandb:     test_forces_mse â–
wandb:    test_forces_rmse â–
wandb:           test_loss â–
wandb:          train_loss â–ˆâ–…â–ƒâ–‚â–ˆâ–ƒâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    val_energy_%rmse â–ˆâ–…â–ƒâ–ƒâ–†â–ƒâ–‚â–‚â–„â–ƒâ–â–„â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mae â–ˆâ–…â–ƒâ–ƒâ–†â–„â–‚â–‚â–„â–ƒâ–â–„â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mse â–ˆâ–ƒâ–‚â–‚â–„â–‚â–â–â–ƒâ–‚â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_energy_rmse â–ˆâ–…â–ƒâ–ƒâ–†â–ƒâ–‚â–‚â–„â–ƒâ–â–„â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val_forces_%rmse â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_forces_mae â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_forces_mse â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_forces_rmse â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val_loss â–ˆâ–ƒâ–‚â–‚â–„â–‚â–â–â–ƒâ–‚â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 500
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.0001
wandb:     test_energy_mae 0.09436
wandb:     test_energy_mse 0.03222
wandb:    test_energy_rmse 0.17949
wandb:   test_forces_%rmse 0.12473
wandb:     test_forces_mae 0.056
wandb:     test_forces_mse 0.01013
wandb:    test_forces_rmse 0.10066
wandb:           test_loss 0.04235
wandb:          train_loss 0.00139
wandb: trainer/global_step 273500
wandb:    val_energy_%rmse 8e-05
wandb:      val_energy_mae 0.08074
wandb:      val_energy_mse 0.02099
wandb:     val_energy_rmse 0.14489
wandb:    val_forces_%rmse 0.11042
wandb:      val_forces_mae 0.05154
wandb:      val_forces_mse 0.0073
wandb:     val_forces_rmse 0.08545
wandb:            val_loss 0.02829
wandb: 
wandb: ğŸš€ View run 6 at: https://wandb.ai/bananenpampe/LiPS-mse-ens/runs/3bqq1w2g
wandb: ï¸âš¡ View job at https://wandb.ai/bananenpampe/LiPS-mse-ens/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTMzMDcwOQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231009_094643-3bqq1w2g/logs
