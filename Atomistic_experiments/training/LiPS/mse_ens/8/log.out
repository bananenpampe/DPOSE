/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231009_094651-rd7o9g0a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 8
wandb: â­ï¸ View project at https://wandb.ai/bananenpampe/LiPS-mse-ens
wandb: ğŸš€ View run at https://wandb.ai/bananenpampe/LiPS-mse-ens/runs/rd7o9g0a
seed 8
/home/kellner/BPNN_packages/H2O/fit_LiPS/mse_ensemble/8/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=3)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=15)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=16)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceRespone()
  )
  (loss_fn): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 194 K 
1 | loss_fn            | EnergyForceLoss        | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 3     
--------------------------------------------------------------
194 K     Trainable params
3         Non-trainable params
194 K     Total params
0.780     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00064: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00136: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00158: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00306: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00333: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00384: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00405: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00426: reducing learning rate of group 0 to 3.9063e-05.
Epoch 00447: reducing learning rate of group 0 to 1.9531e-05.
Epoch 00468: reducing learning rate of group 0 to 9.7656e-06.
Epoch 00489: reducing learning rate of group 0 to 4.8828e-06.
`Trainer.fit` stopped: `max_epochs=500` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Test metric             DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    test_energy_%rmse     0.00010038561502850532
     test_energy_mae        0.09209497063146088
     test_energy_mse       0.031824117082642045
    test_energy_rmse        0.17839315312713672
    test_forces_%rmse       0.12982365168826734
     test_forces_mae        0.05800009695086027
     test_forces_mse        0.01097613501149256
    test_forces_rmse        0.10476705117303131
        test_loss           0.04280025209413461
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   test_energy_%rmse â–
wandb:     test_energy_mae â–
wandb:     test_energy_mse â–
wandb:    test_energy_rmse â–
wandb:   test_forces_%rmse â–
wandb:     test_forces_mae â–
wandb:     test_forces_mse â–
wandb:    test_forces_rmse â–
wandb:           test_loss â–
wandb:          train_loss â–‡â–ˆâ–â–‚â–‚â–ƒâ–‚â–„â–‚â–ƒâ–â–â–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–â–â–ƒâ–‚â–â–â–‚â–‚â–ƒâ–‚â–‚â–‚
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    val_energy_%rmse â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mae â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–ƒâ–â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mse â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_energy_rmse â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val_forces_%rmse â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_forces_mae â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_forces_mse â–ˆâ–…â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_forces_rmse â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val_loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 500
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.0001
wandb:     test_energy_mae 0.09209
wandb:     test_energy_mse 0.03182
wandb:    test_energy_rmse 0.17839
wandb:   test_forces_%rmse 0.12982
wandb:     test_forces_mae 0.058
wandb:     test_forces_mse 0.01098
wandb:    test_forces_rmse 0.10477
wandb:           test_loss 0.0428
wandb:          train_loss 0.00323
wandb: trainer/global_step 273500
wandb:    val_energy_%rmse 9e-05
wandb:      val_energy_mae 0.08443
wandb:      val_energy_mse 0.02499
wandb:     val_energy_rmse 0.15807
wandb:    val_forces_%rmse 0.10966
wandb:      val_forces_mae 0.05224
wandb:      val_forces_mse 0.0072
wandb:     val_forces_rmse 0.08486
wandb:            val_loss 0.03219
wandb: 
wandb: ğŸš€ View run 8 at: https://wandb.ai/bananenpampe/LiPS-mse-ens/runs/rd7o9g0a
wandb: ï¸âš¡ View job at https://wandb.ai/bananenpampe/LiPS-mse-ens/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTMzMDU3MQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231009_094651-rd7o9g0a/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 285, in check_stop_status
    self._loop_check_status(
Exception in thread   File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 223, in _loop_check_status
IntMsgThr:
Traceback (most recent call last):
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    local_handle = request()
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 727, in deliver_stop_status
    self.run()
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 299, in check_internal_messages
    self._loop_check_status(
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 223, in _loop_check_status
    local_handle = request()
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 743, in deliver_internal_messages
        return self._deliver_stop_status(status)return self._deliver_internal_messages(internal_message)

  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 450, in _deliver_stop_status
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 472, in _deliver_internal_messages
        return self._deliver_record(record)return self._deliver_record(record)

  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 425, in _deliver_record
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 425, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
        interface._publish(record)interface._publish(record)

  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)    
self._sock_client.send_record_publish(record)  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish

  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
        self.send_server_request(server_req)self.send_server_request(server_req)

  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
      File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
self._send_message(msg)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)    
self._sendall_with_error_handle(header + data)  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle

  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
    BrokenPipeErrorsent = self._sock.send(data): 
[Errno 32] Broken pipeBrokenPipeError
: [Errno 32] Broken pipe
