/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231009_094701-0o57i17y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 10
wandb: ⭐️ View project at https://wandb.ai/bananenpampe/LiPS-mse-ens
wandb: 🚀 View run at https://wandb.ai/bananenpampe/LiPS-mse-ens/runs/0o57i17y
seed 10
/home/kellner/BPNN_packages/H2O/fit_LiPS/mse_ensemble/10/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=3)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=15)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=16)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceRespone()
  )
  (loss_fn): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 194 K 
1 | loss_fn            | EnergyForceLoss        | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 3     
--------------------------------------------------------------
194 K     Trainable params
3         Non-trainable params
194 K     Total params
0.780     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00104: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00130: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00163: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00184: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00205: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00226: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00247: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00268: reducing learning rate of group 0 to 3.9063e-05.
Epoch 00289: reducing learning rate of group 0 to 1.9531e-05.
Epoch 00310: reducing learning rate of group 0 to 9.7656e-06.
Epoch 00331: reducing learning rate of group 0 to 4.8828e-06.
Epoch 00352: reducing learning rate of group 0 to 2.4414e-06.
Epoch 00373: reducing learning rate of group 0 to 1.2207e-06.
Epoch 00394: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=500` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
    test_energy_%rmse     0.00011848866806626047
     test_energy_mae        0.09923567051816207
     test_energy_mse       0.044337074186639465
    test_energy_rmse        0.2105637057677307
    test_forces_%rmse       0.12694995964634093
     test_forces_mae       0.057032976198068686
     test_forces_mse       0.010495591836779993
    test_forces_rmse        0.1024479957675112
        test_loss          0.054832666023419456
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam █████████▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   test_energy_%rmse ▁
wandb:     test_energy_mae ▁
wandb:     test_energy_mse ▁
wandb:    test_energy_rmse ▁
wandb:   test_forces_%rmse ▁
wandb:     test_forces_mae ▁
wandb:     test_forces_mse ▁
wandb:    test_forces_rmse ▁
wandb:           test_loss ▁
wandb:          train_loss ▄▃▂▆█▂▄▆▄▂▂▃▃▃▂▃▂▂▁▃▁▃▂▂▂▃▂▂▂▁▄▂▄▂▂▄▃▂▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:    val_energy_%rmse ▆▃▅█▄▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mae ▅▂▅█▄▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mse ▅▂▄█▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_energy_rmse ▆▃▅█▄▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val_forces_%rmse █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mae █▅▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mse █▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_forces_rmse █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_loss ▅▂▄█▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 500
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.00012
wandb:     test_energy_mae 0.09924
wandb:     test_energy_mse 0.04434
wandb:    test_energy_rmse 0.21056
wandb:   test_forces_%rmse 0.12695
wandb:     test_forces_mae 0.05703
wandb:     test_forces_mse 0.0105
wandb:    test_forces_rmse 0.10245
wandb:           test_loss 0.05483
wandb:          train_loss 0.00107
wandb: trainer/global_step 273500
wandb:    val_energy_%rmse 9e-05
wandb:      val_energy_mae 0.08603
wandb:      val_energy_mse 0.0259
wandb:     val_energy_rmse 0.16093
wandb:    val_forces_%rmse 0.11298
wandb:      val_forces_mae 0.05228
wandb:      val_forces_mse 0.00764
wandb:     val_forces_rmse 0.08743
wandb:            val_loss 0.03354
wandb: 
wandb: 🚀 View run 10 at: https://wandb.ai/bananenpampe/LiPS-mse-ens/runs/0o57i17y
wandb: ️⚡ View job at https://wandb.ai/bananenpampe/LiPS-mse-ens/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTMzMDA3OA==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231009_094701-0o57i17y/logs
