/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231003_222632-rmc94zhp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run first-run
wandb: ⭐️ View project at https://wandb.ai/bananenpampe/LiPS
wandb: 🚀 View run at https://wandb.ai/bananenpampe/LiPS/runs/rmc94zhp
seed 1
/home/kellner/BPNN_packages/H2O/fit_LiPS/run_1/../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=3)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=64, bias=True)
          )
          (LabelsEntry(species_center=15)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=64, bias=True)
          )
          (LabelsEntry(species_center=16)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=948, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=64, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceUncertaintyRespone()
  )
  (loss_fn): EnergyForceUncertaintyLoss(
    (energy_loss): GaussianNLLLoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                       | Params
------------------------------------------------------------------
0 | model              | BPNNModel                  | 207 K 
1 | loss_fn            | EnergyForceUncertaintyLoss | 0     
2 | loss_rmse          | EnergyForceLoss            | 0     
3 | loss_mae           | EnergyForceLoss            | 0     
4 | energy_transformer | CompositionTransformer     | 3     
------------------------------------------------------------------
207 K     Trainable params
3         Non-trainable params
207 K     Total params
0.829     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00250: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00275: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00297: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00318: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00339: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00360: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00381: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00402: reducing learning rate of group 0 to 3.9063e-05.
Epoch 00423: reducing learning rate of group 0 to 1.9531e-05.
Epoch 00444: reducing learning rate of group 0 to 9.7656e-06.
Epoch 00465: reducing learning rate of group 0 to 4.8828e-06.
Epoch 00486: reducing learning rate of group 0 to 2.4414e-06.
Epoch 00507: reducing learning rate of group 0 to 1.2207e-06.
Epoch 00528: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=1000` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
    test_energy_%rmse     0.00012006509293408069
     test_energy_mae        0.08629507949527238
     test_energy_mse        0.0455246817109489
    test_energy_rmse        0.21336513705605445
    test_forces_%rmse       0.13446038879150496
     test_forces_mae        0.0575498902656195
     test_forces_mse       0.011774176020931159
    test_forces_rmse        0.10850887530949327
        test_loss           0.05729885773188006
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ██████████▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   test_energy_%rmse ▁
wandb:     test_energy_mae ▁
wandb:     test_energy_mse ▁
wandb:    test_energy_rmse ▁
wandb:   test_forces_%rmse ▁
wandb:     test_forces_mae ▁
wandb:     test_forces_mse ▁
wandb:    test_forces_rmse ▁
wandb:           test_loss ▁
wandb:          train_loss █▅▄▃▅▃▂▃▃▃▂▂▂▂▂▁▂▁▁▁▂▃▂▃▂▂▃▁▂▂▁▁▂▂▂▁▃▂▁▂
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:    val_energy_%rmse ▂▁▂▂▁█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mae ▂▁▂▂▁█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mse ▁▁▁▁▁█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_energy_rmse ▂▁▂▂▁█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val_forces_%rmse █▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mae █▄▄▃▃▂▃▂▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mse █▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_forces_rmse █▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_loss ▁▁▁▁▁█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 1000
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.00012
wandb:     test_energy_mae 0.0863
wandb:     test_energy_mse 0.04552
wandb:    test_energy_rmse 0.21337
wandb:   test_forces_%rmse 0.13446
wandb:     test_forces_mae 0.05755
wandb:     test_forces_mse 0.01177
wandb:    test_forces_rmse 0.10851
wandb:           test_loss 0.0573
wandb:          train_loss -0.00133
wandb: trainer/global_step 547000
wandb:    val_energy_%rmse 0.0001
wandb:      val_energy_mae 0.08198
wandb:      val_energy_mse 0.02999
wandb:     val_energy_rmse 0.17317
wandb:    val_forces_%rmse 0.11147
wandb:      val_forces_mae 0.05214
wandb:      val_forces_mse 0.00744
wandb:     val_forces_rmse 0.08626
wandb:            val_loss 0.03743
wandb: 
wandb: 🚀 View run first-run at: https://wandb.ai/bananenpampe/LiPS/runs/rmc94zhp
wandb: ️⚡ View job at https://wandb.ai/bananenpampe/LiPS/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNDAxMDQyOQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231003_222632-rmc94zhp/logs
