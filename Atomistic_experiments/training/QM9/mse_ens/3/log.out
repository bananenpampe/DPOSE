wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231002_020939-9cy4e06q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3
wandb: â­ï¸ View project at https://wandb.ai/bananenpampe/QM9-mse-ensemble
wandb: ğŸš€ View run at https://wandb.ai/bananenpampe/QM9-mse-ensemble/runs/9cy4e06q
seed 3
/home/kellner/BPNN_packages/H2O/QM9_train/deepens_mse/3/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 16.3 M
1 | loss_fn            | GeneralLoss            | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 5     
--------------------------------------------------------------
16.3 M    Trainable params
5         Non-trainable params
16.3 M    Total params
65.260    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 00120: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00180: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00239: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00358: reducing learning rate of group 0 to 6.2500e-05.
Epoch 00382: reducing learning rate of group 0 to 3.1250e-05.
Epoch 00439: reducing learning rate of group 0 to 1.5625e-05.
`Trainer.fit` stopped: `max_epochs=500` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Test metric             DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     test_energy_mae       0.001260694404794049
     test_energy_mse       4.10790035301721e-06
    test_energy_rmse       0.002026795587378562
        test_loss          4.10790035301721e-06
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     test_energy_mae â–
wandb:     test_energy_mse â–
wandb:    test_energy_rmse â–
wandb:           test_loss â–
wandb:          train_loss â–†â–…â–„â–ˆâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      val_energy_mae â–ˆâ–…â–„â–…â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mse â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_energy_rmse â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val_loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 500
wandb:             lr-Adam 2e-05
wandb:     test_energy_mae 0.00126
wandb:     test_energy_mse 0.0
wandb:    test_energy_rmse 0.00203
wandb:           test_loss 0.0
wandb:          train_loss 0.0
wandb: trainer/global_step 312500
wandb:      val_energy_mae 0.00124
wandb:      val_energy_mse 0.0
wandb:     val_energy_rmse 0.00181
wandb:            val_loss 0.0
wandb: 
wandb: ğŸš€ View run 3 at: https://wandb.ai/bananenpampe/QM9-mse-ensemble/runs/9cy4e06q
wandb: ï¸âš¡ View job at https://wandb.ai/bananenpampe/QM9-mse-ensemble/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNDc5ODU0MA==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231002_020939-9cy4e06q/logs
