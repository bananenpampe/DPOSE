/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20230918_020730-6jujp07q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 9
wandb: â­ï¸ View project at https://wandb.ai/bananenpampe/H2O-mse-mean-only-ensemble
wandb: ğŸš€ View run at https://wandb.ai/bananenpampe/H2O-mse-mean-only-ensemble/runs/6jujp07q
seed 9
/home/kellner/BPNN_packages/H2O/example/mse_commitee/9/../../../model/transformer/composition.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
bias True
bias True
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=1)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=482, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=8)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=482, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceRespone()
  )
  (loss_fn): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 70.3 K
1 | loss_fn            | EnergyForceLoss        | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 2     
--------------------------------------------------------------
70.3 K    Trainable params
2         Non-trainable params
70.3 K    Total params
0.281     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-izifuv1s/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00093: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00179: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00266: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00301: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00327: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00350: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00375: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00391: reducing learning rate of group 0 to 3.9063e-05.
Epoch 00407: reducing learning rate of group 0 to 1.9531e-05.
Epoch 00423: reducing learning rate of group 0 to 9.7656e-06.
Epoch 00439: reducing learning rate of group 0 to 4.8828e-06.
Epoch 00455: reducing learning rate of group 0 to 2.4414e-06.
Epoch 00471: reducing learning rate of group 0 to 1.2207e-06.
Epoch 00487: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=500` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Test metric             DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    test_energy_%rmse       0.02740541057115014
     test_energy_mae        0.23073960965596144
     test_energy_mse        0.13516773168663138
    test_energy_rmse         0.367651644476985
    test_forces_%rmse       0.03279901404165476
     test_forces_mae       0.041726337493462974
     test_forces_mse       0.005031040442584244
    test_forces_rmse        0.0709298275944912
        test_loss           0.1401987721292156
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   test_energy_%rmse â–
wandb:     test_energy_mae â–
wandb:     test_energy_mse â–
wandb:    test_energy_rmse â–
wandb:   test_forces_%rmse â–
wandb:     test_forces_mae â–
wandb:     test_forces_mse â–
wandb:    test_forces_rmse â–
wandb:           test_loss â–
wandb:          train_loss â–ˆâ–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–‚
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    val_energy_%rmse â–ˆâ–…â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mae â–ˆâ–…â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_energy_mse â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_energy_rmse â–ˆâ–…â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val_forces_%rmse â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_forces_mae â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      val_forces_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     val_forces_rmse â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val_loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 500
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.02741
wandb:     test_energy_mae 0.23074
wandb:     test_energy_mse 0.13517
wandb:    test_energy_rmse 0.36765
wandb:   test_forces_%rmse 0.0328
wandb:     test_forces_mae 0.04173
wandb:     test_forces_mse 0.00503
wandb:    test_forces_rmse 0.07093
wandb:           test_loss 0.1402
wandb:          train_loss 0.00157
wandb: trainer/global_step 159500
wandb:    val_energy_%rmse 0.0277
wandb:      val_energy_mae 0.26227
wandb:      val_energy_mse 0.16471
wandb:     val_energy_rmse 0.40584
wandb:    val_forces_%rmse 0.03061
wandb:      val_forces_mae 0.04164
wandb:      val_forces_mse 0.00465
wandb:     val_forces_rmse 0.06817
wandb:            val_loss 0.16935
wandb: 
wandb: ğŸš€ View run 9 at: https://wandb.ai/bananenpampe/H2O-mse-mean-only-ensemble/runs/6jujp07q
wandb: ï¸âš¡ View job at https://wandb.ai/bananenpampe/H2O-mse-mean-only-ensemble/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk4NjYyMTE1/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230918_020730-6jujp07q/logs
