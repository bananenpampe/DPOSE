/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231022_133315-ekqr4nbn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 8
wandb: ⭐️ View project at https://wandb.ai/bananenpampe/BaTiO3-mse-ens
wandb: 🚀 View run at https://wandb.ai/bananenpampe/BaTiO3-mse-ens/runs/ekqr4nbn
seed 8
/home/kellner/BPNN_packages/H2O/BaTiO3/mse_ensemble/8/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=8)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=22)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=56)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceRespone()
  )
  (loss_fn): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 312 K 
1 | loss_fn            | EnergyForceLoss        | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 3     
--------------------------------------------------------------
312 K     Trainable params
3         Non-trainable params
312 K     Total params
1.250     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00288: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00509: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00815: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00973: reducing learning rate of group 0 to 6.2500e-04.
Epoch 01045: reducing learning rate of group 0 to 3.1250e-04.
Epoch 01086: reducing learning rate of group 0 to 1.5625e-04.
Epoch 01107: reducing learning rate of group 0 to 7.8125e-05.
Epoch 01128: reducing learning rate of group 0 to 3.9063e-05.
Epoch 01149: reducing learning rate of group 0 to 1.9531e-05.
Epoch 01170: reducing learning rate of group 0 to 9.7656e-06.
Epoch 01191: reducing learning rate of group 0 to 4.8828e-06.
Epoch 01212: reducing learning rate of group 0 to 2.4414e-06.
Epoch 01233: reducing learning rate of group 0 to 1.2207e-06.
Epoch 01254: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=2000` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
    test_energy_%rmse      0.006318126575023556
     test_energy_mae       0.008045035048366393
     test_energy_mse      0.00013399136234914586
    test_energy_rmse        0.01157546380708548
    test_forces_%rmse      0.020055155487353638
     test_forces_mae       0.0039029537732456153
     test_forces_mse      0.00011808910659380812
    test_forces_rmse       0.010866881180624371
        test_loss          0.000252080468942954
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ██████▄▄▄▄▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   test_energy_%rmse ▁
wandb:     test_energy_mae ▁
wandb:     test_energy_mse ▁
wandb:    test_energy_rmse ▁
wandb:   test_forces_%rmse ▁
wandb:     test_forces_mae ▁
wandb:     test_forces_mse ▁
wandb:    test_forces_rmse ▁
wandb:           test_loss ▁
wandb:          train_loss ▃▂▂▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▃▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:    val_energy_%rmse █▂▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mae █▂▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mse █▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_energy_rmse █▂▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val_forces_%rmse █▃▂▂▂▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mae ▆▄▃▂▃█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mse █▂▂▁▂▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_forces_rmse █▃▂▂▂▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_loss █▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 2000
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.00632
wandb:     test_energy_mae 0.00805
wandb:     test_energy_mse 0.00013
wandb:    test_energy_rmse 0.01158
wandb:   test_forces_%rmse 0.02006
wandb:     test_forces_mae 0.0039
wandb:     test_forces_mse 0.00012
wandb:    test_forces_rmse 0.01087
wandb:           test_loss 0.00025
wandb:          train_loss 0.00029
wandb: trainer/global_step 600000
wandb:    val_energy_%rmse 0.00612
wandb:      val_energy_mae 0.00886
wandb:      val_energy_mse 0.00025
wandb:     val_energy_rmse 0.01596
wandb:    val_forces_%rmse 0.0225
wandb:      val_forces_mae 0.00535
wandb:      val_forces_mse 0.00022
wandb:     val_forces_rmse 0.01489
wandb:            val_loss 0.00048
wandb: 
wandb: 🚀 View run 8 at: https://wandb.ai/bananenpampe/BaTiO3-mse-ens/runs/ekqr4nbn
wandb: ️⚡ View job at https://wandb.ai/bananenpampe/BaTiO3-mse-ens/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwODc4MzQ5Nw==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231022_133315-ekqr4nbn/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 267, in check_network_status
    self._loop_check_status(
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 223, in _loop_check_status
    local_handle = request()
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 735, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 466, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 425, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
