/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231022_133315-u1yl8mkp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 3
wandb: ⭐️ View project at https://wandb.ai/bananenpampe/BaTiO3-mse-ens
wandb: 🚀 View run at https://wandb.ai/bananenpampe/BaTiO3-mse-ens/runs/u1yl8mkp
seed 3
/home/kellner/BPNN_packages/H2O/BaTiO3/mse_ensemble/3/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=8)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=22)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
          (LabelsEntry(species_center=56)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=1, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceRespone()
  )
  (loss_fn): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                   | Params
--------------------------------------------------------------
0 | model              | BPNNModel              | 312 K 
1 | loss_fn            | EnergyForceLoss        | 0     
2 | loss_rmse          | EnergyForceLoss        | 0     
3 | loss_mae           | EnergyForceLoss        | 0     
4 | energy_transformer | CompositionTransformer | 3     
--------------------------------------------------------------
312 K     Trainable params
3         Non-trainable params
312 K     Total params
1.250     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00206: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00507: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00664: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00845: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00889: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00922: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00961: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00982: reducing learning rate of group 0 to 3.9063e-05.
Epoch 01003: reducing learning rate of group 0 to 1.9531e-05.
Epoch 01024: reducing learning rate of group 0 to 9.7656e-06.
Epoch 01045: reducing learning rate of group 0 to 4.8828e-06.
Epoch 01066: reducing learning rate of group 0 to 2.4414e-06.
Epoch 01087: reducing learning rate of group 0 to 1.2207e-06.
Epoch 01108: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=2000` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
    test_energy_%rmse      0.005438807569741698
     test_energy_mae       0.006719321095387604
     test_energy_mse       9.929046512762845e-05
    test_energy_rmse       0.009964460102164514
    test_forces_%rmse       0.02133740924014245
     test_forces_mae       0.0040893933323836226
     test_forces_mse      0.00013367221477023666
    test_forces_rmse       0.011561670068387035
        test_loss         0.00023296267989786514
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ████▄▄▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   test_energy_%rmse ▁
wandb:     test_energy_mae ▁
wandb:     test_energy_mse ▁
wandb:    test_energy_rmse ▁
wandb:   test_forces_%rmse ▁
wandb:     test_forces_mae ▁
wandb:     test_forces_mse ▁
wandb:    test_forces_rmse ▁
wandb:           test_loss ▁
wandb:          train_loss ▁▂█▁▁▁▁▁▁▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▃▁▁▂▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:    val_energy_%rmse █▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mae █▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_energy_rmse █▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val_forces_%rmse █▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mae █▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mse █▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_forces_rmse █▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 2000
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.00544
wandb:     test_energy_mae 0.00672
wandb:     test_energy_mse 0.0001
wandb:    test_energy_rmse 0.00996
wandb:   test_forces_%rmse 0.02134
wandb:     test_forces_mae 0.00409
wandb:     test_forces_mse 0.00013
wandb:    test_forces_rmse 0.01156
wandb:           test_loss 0.00023
wandb:          train_loss 1e-05
wandb: trainer/global_step 600000
wandb:    val_energy_%rmse 0.00534
wandb:      val_energy_mae 0.00706
wandb:      val_energy_mse 0.00019
wandb:     val_energy_rmse 0.01392
wandb:    val_forces_%rmse 0.02472
wandb:      val_forces_mae 0.00572
wandb:      val_forces_mse 0.00027
wandb:     val_forces_rmse 0.01636
wandb:            val_loss 0.00046
wandb: 
wandb: 🚀 View run 3 at: https://wandb.ai/bananenpampe/BaTiO3-mse-ens/runs/u1yl8mkp
wandb: ️⚡ View job at https://wandb.ai/bananenpampe/BaTiO3-mse-ens/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwODc4MjEwNw==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231022_133315-u1yl8mkp/logs
