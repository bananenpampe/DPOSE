/home/kellner/BPNN_packages/H2O/model/dataset/../equisolve_futures/convert_torch.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  values=torch.tensor(gradient_values.reshape(-1, 3, 1)),
wandb: Currently logged in as: bananenpampe. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/kellner/.netrc
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in ./wandb/run-20231024_121856-pulr9tut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1
wandb: ⭐️ View project at https://wandb.ai/bananenpampe/BaTiO3-deepens-crps
wandb: 🚀 View run at https://wandb.ai/bananenpampe/BaTiO3-deepens-crps/runs/pulr9tut
seed 1
/home/kellner/BPNN_packages/H2O/BaTiO3/deep_ensemble_CRPS/1/../../../model/transformer/composition.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  unique_labels = torch.tensor(self.unique_species, dtype=torch.int32).reshape(-1,1)
BPNNRascalineModule(
  (model): BPNNModel(
    (feature): UnitFeatures()
    (interaction): BPNNInteraction(
      (model): metatensorMLPLazy(
        (m_map): ModuleDict(
          (LabelsEntry(species_center=8)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=64, bias=True)
          )
          (LabelsEntry(species_center=22)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=64, bias=True)
          )
          (LabelsEntry(species_center=56)): MLP_mean(
            (nn): Sequential(
              (0): Linear(in_features=1560, out_features=64, bias=True)
              (1): SiLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): SiLU()
            )
            (mean_out): Linear(in_features=64, out_features=64, bias=True)
          )
        )
      )
    )
    (aggregation): BPNNStructureWiseAggregation()
    (response): ForceUncertaintyRespone()
  )
  (loss_fn): EnergyForceUncertaintyLoss(
    (energy_loss): CRPS()
    (force_loss): MSELoss()
  )
  (loss_rmse): EnergyForceLoss(
    (energy_loss): MSELoss()
    (force_loss): MSELoss()
  )
  (loss_mae): EnergyForceLoss(
    (energy_loss): L1Loss()
    (force_loss): L1Loss()
  )
  (energy_transformer): CompositionTransformer(
    (calc): AtomicComposition()
  )
)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

  | Name               | Type                       | Params
------------------------------------------------------------------
0 | model              | BPNNModel                  | 324 K 
1 | loss_fn            | EnergyForceUncertaintyLoss | 0     
2 | loss_rmse          | EnergyForceLoss            | 0     
3 | loss_mae           | EnergyForceLoss            | 0     
4 | energy_transformer | CompositionTransformer     | 3     
------------------------------------------------------------------
324 K     Trainable params
3         Non-trainable params
324 K     Total params
1.299     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: second derivatives with respect to positions are not implemented and will not be accumulated during backward() calls. If you need second derivatives, please open an issue on rascaline repository. (Triggered internally at /tmp/pip-req-build-wdjg9k6b/rascaline-torch/src/autograd.cpp:314.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00133: reducing learning rate of group 0 to 5.0000e-03.
Epoch 00296: reducing learning rate of group 0 to 2.5000e-03.
Epoch 00515: reducing learning rate of group 0 to 1.2500e-03.
Epoch 00543: reducing learning rate of group 0 to 6.2500e-04.
Epoch 00590: reducing learning rate of group 0 to 3.1250e-04.
Epoch 00669: reducing learning rate of group 0 to 1.5625e-04.
Epoch 00690: reducing learning rate of group 0 to 7.8125e-05.
Epoch 00711: reducing learning rate of group 0 to 3.9063e-05.
Epoch 00732: reducing learning rate of group 0 to 1.9531e-05.
Epoch 00753: reducing learning rate of group 0 to 9.7656e-06.
Epoch 00774: reducing learning rate of group 0 to 4.8828e-06.
Epoch 00795: reducing learning rate of group 0 to 2.4414e-06.
Epoch 00816: reducing learning rate of group 0 to 1.2207e-06.
Epoch 00837: reducing learning rate of group 0 to 1.0000e-06.
`Trainer.fit` stopped: `max_epochs=2000` reached.
SLURM auto-requeueing enabled. Setting signal handlers.
/home/kellner/miniconda3/envs/metatens/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
    test_energy_%rmse      0.006641669398964548
     test_energy_mae        0.00731504249708013
     test_energy_mse      0.00014806576899531983
    test_energy_rmse       0.012168227849416687
    test_forces_%rmse      0.026267515475445338
     test_forces_mae       0.005502229274841341
     test_forces_mse       0.0002025796253969075
    test_forces_rmse       0.014233046947049235
        test_loss         0.00035064539439222733
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ███▄▄▄▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   test_energy_%rmse ▁
wandb:     test_energy_mae ▁
wandb:     test_energy_mse ▁
wandb:    test_energy_rmse ▁
wandb:   test_forces_%rmse ▁
wandb:     test_forces_mae ▁
wandb:     test_forces_mse ▁
wandb:    test_forces_rmse ▁
wandb:           test_loss ▁
wandb:          train_loss ▅▆▅▄▂▂▁▇▂▂▁▁▁▁▁▁▂▁▁▁█▁▁▂▁▁▆▁▁▄▁▁▁▁▁▁▂▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:    val_energy_%rmse ▄▇█▂▃▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mae ▃▇█▂▃▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_energy_mse ▂▇█▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_energy_rmse ▄▇█▂▃▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    val_forces_%rmse █▇▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mae ▇█▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      val_forces_mse █▇▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_forces_rmse █▇▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_loss ▂▇█▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 2000
wandb:             lr-Adam 0.0
wandb:   test_energy_%rmse 0.00664
wandb:     test_energy_mae 0.00732
wandb:     test_energy_mse 0.00015
wandb:    test_energy_rmse 0.01217
wandb:   test_forces_%rmse 0.02627
wandb:     test_forces_mae 0.0055
wandb:     test_forces_mse 0.0002
wandb:    test_forces_rmse 0.01423
wandb:           test_loss 0.00035
wandb:          train_loss 1e-05
wandb: trainer/global_step 600000
wandb:    val_energy_%rmse 0.00638
wandb:      val_energy_mae 0.00749
wandb:      val_energy_mse 0.00028
wandb:     val_energy_rmse 0.01665
wandb:    val_forces_%rmse 0.03048
wandb:      val_forces_mae 0.00747
wandb:      val_forces_mse 0.00041
wandb:     val_forces_rmse 0.02017
wandb:            val_loss 0.00068
wandb: 
wandb: 🚀 View run 1 at: https://wandb.ai/bananenpampe/BaTiO3-deepens-crps/runs/pulr9tut
wandb: ️⚡ View job at https://wandb.ai/bananenpampe/BaTiO3-deepens-crps/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwOTM4MDc4Ng==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231024_121856-pulr9tut/logs
