{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook summarizes the proposed approach to uncertainty quantification \\\n",
    "and propagation for atomistic modelling. \\\n",
    "A common approach for uncertainty quantification of neural network model predictions is \\\n",
    "to directly estimate the target mean $\\mu$ and variance $\\sigma^2$ using a neural network. [Nix and Weigend 1994](https://doi.org/10.1109/ICNN.1994.374138)\\\n",
    "\\\n",
    "Machine learning models / neural networks for the modelling of atomistic properties, \\\n",
    "typically aim to predict some structure-wise property, e.g. the energy of a molecule. \\\n",
    "The global predictions are typically obtained by summing over atom-wise local predictions. \\\n",
    "This choice is typically made, to ensure that the model can be applied to systems of different sizes \\\n",
    "and ensures linear scaling of the computational evaluation cost of the model with system size. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVE(torch.nn.Module):\n",
    "    \"\"\" Mean variance estimator\n",
    "\n",
    "    A neural network that predicts both mean and variance for a given input.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_out=2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # n_out is the number of output neurons\n",
    "        # in this case we predict both mean and variance\n",
    "\n",
    "        self.nn = torch.nn.Sequential(\n",
    "                torch.nn.Linear(1, 64),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(64, n_out),\n",
    "                )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        out = self.nn(x)\n",
    "        mean = out[:,0]\n",
    "        var = out[:,1]\n",
    "\n",
    "        # forces predicted variance to be positive \n",
    "        # by applying a softplus to the NN output\n",
    "        # alternatively the log variances could be directly predicted\n",
    "\n",
    "        var = torch.nn.functional.softplus(var)\n",
    "\n",
    "        return mean, var\n",
    "\n",
    "\n",
    "class ShallowEnsemble(MVE):\n",
    "    \"\"\" Shallow ensemble model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(n_out=64)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.nn(x)\n",
    "        mean = torch.mean(out axis=1)\n",
    "        var = torch.var(out, axis=1)\n",
    "        \n",
    "        return mean, var\n",
    "\n",
    "    def return_committee(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.nn(x)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(x):\n",
    "    return x**2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
