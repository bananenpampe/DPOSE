{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../UCI_experiments/')\n",
    "\n",
    "import torch\n",
    "from model.metrics import MAE, MSE, RMSE, get_coeff, NLL\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from uncertainty_toolbox.metrics_calibration import miscalibration_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def NLL(input: torch.Tensor, target: torch.Tensor, var: torch.Tensor, full=True, eps=1e-06):\n",
    "    return torch.nn.functional.gaussian_nll_loss(input.flatten(), target.flatten(), var.flatten(), full=full, eps=eps)\n",
    "\n",
    "def RMSE(input, target):\n",
    "    return torch.sqrt(torch.mean((input.flatten() - target.flatten())**2))\n",
    "\n",
    "def MSE(input, target):\n",
    "    return torch.mean((input.flatten() - target.flatten())**2)\n",
    "\n",
    "def MAE(input, target):\n",
    "    return torch.mean(torch.abs((input.flatten() - target.flatten())))\n",
    "\n",
    "#use zero eps everywhere -> was not necessary during training, so no addition should be necessary either\n",
    "def get_coeff(input: torch.Tensor, target: torch.Tensor, var: torch.Tensor) -> torch.tensor:\n",
    "    \"\"\" Returns dimensionless NLL coefficient\n",
    "    \"\"\"\n",
    "    \n",
    "    mse = MSE(input,target)\n",
    "    uncertainty_estimate = (input.flatten() - target.flatten())**2\n",
    "    \n",
    "    LL_best = torch.nn.functional.gaussian_nll_loss(input.flatten(), target.flatten(), uncertainty_estimate.flatten(), full=False, eps=0.)\n",
    "    \n",
    "    LL_worst_case_best_RMSE = torch.nn.functional.gaussian_nll_loss\\\n",
    "        (input.flatten(), target.flatten(), torch.ones_like(var.flatten())*mse, full=False, eps=0.)\n",
    "    \n",
    "    LL_actual = torch.nn.functional.gaussian_nll_loss(input.flatten(), target.flatten(), var.flatten(), full=False, eps=0.)\n",
    "    \n",
    "    coeff = 1/( LL_best - LL_worst_case_best_RMSE) * (LL_actual - LL_worst_case_best_RMSE) * 100\n",
    "\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O shallow_ens MAE: 0.2181 RMSE: 0.4127 alpha: 1.1090, coeff: 59.7128, coeff calibrated: 58.9628, NLL: -0.4135, NLL calibrated: -0.4016\n",
      "H2O shallow_ens MA: 0.0127, MA calibrated: 0.0418\n",
      "H2O mse_ens MAE: 0.2305 RMSE: 0.3689 alpha: 13.8017, coeff: -4610.7609, coeff calibrated: 33.0798, NLL: 58.9034, NLL calibrated: 0.0021\n",
      "H2O mse_ens MA: 0.4064, MA calibrated: 0.0897\n",
      "LiPS shallow_ens MAE: 0.0863 RMSE: 0.2134 alpha: 1.3461, coeff: 55.2924, coeff calibrated: 63.6933, NLL: -1.1814, NLL calibrated: -1.3417\n",
      "LiPS shallow_ens MA: 0.0771, MA calibrated: 0.0245\n",
      "LiPS mse_ens MAE: 0.0920 RMSE: 0.1784 alpha: 4.1100, coeff: -409.8720, coeff calibrated: 58.0604, NLL: 5.4174, NLL calibrated: -1.1153\n",
      "LiPS mse_ens MA: 0.3543, MA calibrated: 0.0126\n",
      "BaTiO3 shallow_ens MAE: 0.0073 RMSE: 0.0122 alpha: 0.4928, coeff: 11.8515, coeff calibrated: 36.6664, NLL: -3.1250, NLL calibrated: -3.4076\n",
      "BaTiO3 shallow_ens MA: 0.2099, MA calibrated: 0.0465\n",
      "BaTiO3 mse_ens MAE: 0.0065 RMSE: 0.0096 alpha: 3.3024, coeff: -428.8267, coeff calibrated: 31.7826, NLL: -0.0251, NLL calibrated: -3.4905\n",
      "BaTiO3 mse_ens MA: 0.3580, MA calibrated: 0.0577\n",
      "QM9 shallow_ens MAE: 0.0263 RMSE: 0.0513 alpha: 1.3598, coeff: 32.8520, coeff calibrated: 41.4183, NLL: -1.9684, NLL calibrated: -2.0771\n",
      "QM9 shallow_ens MA: 0.0560, MA calibrated: 0.0385\n",
      "QM9 mse_ens MAE: 0.0332 RMSE: 0.0542 alpha: 5.6128, coeff: -1198.7976, coeff calibrated: 13.1646, NLL: 10.6869, NLL calibrated: -1.6298\n",
      "QM9 mse_ens MA: 0.3372, MA calibrated: 0.0855\n"
     ]
    }
   ],
   "source": [
    "hartree_to_ev = 27.21138602\n",
    "\n",
    "\n",
    "for identifier in [\"H2O\",\"LiPS\",\"BaTiO3\",\"QM9\"]:\n",
    "    #shallow_ens refers to NLL opt: H2O, LiPS and CRPS opt for BaTiO3 and QM9\n",
    "    for architecture in [\"shallow_ens\",\"mse_ens\"]:\n",
    "        test_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/test_energy.pt').detach().numpy().flatten()\n",
    "        test_pred_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/test_pred_energy.pt').detach().numpy().flatten()\n",
    "        #test_pred_forces = torch.load(f'./{identifier}/{architecture}/test_pred_forces.pt').detach().numpy()\n",
    "        test_pred_energy_var = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/test_pred_energy_var.pt').detach().numpy().flatten()\n",
    "\n",
    "\n",
    "        # now for validation \n",
    "        val_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/val_energy.pt').detach().numpy().flatten()\n",
    "        val_pred_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/val_pred_energy.pt').detach().numpy().flatten()\n",
    "        #val_pred_forces = torch.load(f'./{identifier}/{architecture}/val_pred_forces.pt').detach()\n",
    "        val_pred_energy_var = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/val_pred_energy_var.pt').detach().numpy().flatten()\n",
    "\n",
    "        if identifier == \"QM9\":\n",
    "            test_energy *= hartree_to_ev\n",
    "            test_pred_energy *= hartree_to_ev\n",
    "            test_pred_energy_var *= hartree_to_ev**2\n",
    "            val_energy *= hartree_to_ev\n",
    "            val_pred_energy *= hartree_to_ev\n",
    "            val_pred_energy_var *= hartree_to_ev**2\n",
    "\n",
    "        z_val_split = np.abs(val_energy-val_pred_energy)\n",
    "        alpha = np.sqrt(np.mean(z_val_split**2/val_pred_energy_var, axis=0))\n",
    "\n",
    "        z = np.abs(test_energy- test_pred_energy)\n",
    "\n",
    "        mae = mean_absolute_error(test_energy, test_pred_energy)\n",
    "        mse = mean_squared_error(test_energy, test_pred_energy)\n",
    "        rmse = mean_squared_error(test_energy, test_pred_energy,squared=False)\n",
    "\n",
    "        coef =  get_coeff(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var))\n",
    "        coef_cal = get_coeff(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var)*alpha**2)\n",
    "\n",
    "        ma = miscalibration_area(test_pred_energy.reshape(-1,1), np.sqrt(test_pred_energy_var).reshape(-1,1), test_energy.reshape(-1,1))\n",
    "        ma_cal = miscalibration_area(test_pred_energy.reshape(-1,1), np.sqrt(test_pred_energy_var*alpha**2).reshape(-1,1), test_energy.reshape(-1,1))\n",
    "        \n",
    "        nll = NLL(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var))\n",
    "        nll_cal = NLL(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var)*alpha**2)\n",
    "\n",
    "        print(f'{identifier} {architecture} MAE: {mae:.4f} RMSE: {rmse:.4f} alpha: {alpha:.4f}, coeff: {coef:.4f}, coeff calibrated: {coef_cal:.4f}, NLL: {nll:.4f}, NLL calibrated: {nll_cal:.4f}')\n",
    "        print(f'{identifier} {architecture} MA: {ma:.4f}, MA calibrated: {ma_cal:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaTiO3 shallow_ens_nll MAE: 0.0100 RMSE: 0.0209 alpha: 0.7773, coeff: -29.4067, coeff calibrated: -34.3042, NLL: -2.1349, NLL calibrated: -2.3409\n",
      "BaTiO3 shallow_ens_nll MA: 0.3146, MA calibrated: 0.2881\n",
      "QM9 shallow_ens_nll MAE: 0.0259 RMSE: 0.0529 alpha: 6.1363, coeff: -1089.9793, coeff calibrated: 1.9791, NLL: 12.1404, NLL calibrated: -1.5460\n",
      "QM9 shallow_ens_nll MA: 0.2524, MA calibrated: 0.1836\n"
     ]
    }
   ],
   "source": [
    "hartree_to_ev = 27.21138602\n",
    "\n",
    "\n",
    "for identifier in [\"BaTiO3\",\"QM9\"]:\n",
    "    for architecture in [\"shallow_ens_nll\"]:\n",
    "        test_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/test_energy.pt').detach().numpy().flatten()\n",
    "        test_pred_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/test_pred_energy.pt').detach().numpy().flatten()\n",
    "        #test_pred_forces = torch.load(f'./{identifier}/{architecture}/test_pred_forces.pt').detach().numpy()\n",
    "        test_pred_energy_var = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/test_pred_energy_var.pt').detach().numpy().flatten()\n",
    "\n",
    "\n",
    "        # now for validation \n",
    "        val_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/val_energy.pt').detach().numpy().flatten()\n",
    "        val_pred_energy = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/val_pred_energy.pt').detach().numpy().flatten()\n",
    "        #val_pred_forces = torch.load(f'./{identifier}/{architecture}/val_pred_forces.pt').detach()\n",
    "        val_pred_energy_var = torch.load(f'../../Atomistic_experiments/materials_model_predictions/{identifier}/{architecture}/val_pred_energy_var.pt').detach().numpy().flatten()\n",
    "\n",
    "        if identifier == \"QM9\":\n",
    "            test_energy *= hartree_to_ev\n",
    "            test_pred_energy *= hartree_to_ev\n",
    "            test_pred_energy_var *= hartree_to_ev**2\n",
    "            val_energy *= hartree_to_ev\n",
    "            val_pred_energy *= hartree_to_ev\n",
    "            val_pred_energy_var *= hartree_to_ev**2\n",
    "\n",
    "        z_val_split = np.abs(val_energy-val_pred_energy)\n",
    "        alpha = np.sqrt(np.mean(z_val_split**2/val_pred_energy_var, axis=0))\n",
    "\n",
    "        z = np.abs(test_energy- test_pred_energy)\n",
    "\n",
    "        mae = mean_absolute_error(test_energy, test_pred_energy)\n",
    "        mse = mean_squared_error(test_energy, test_pred_energy)\n",
    "        rmse = mean_squared_error(test_energy, test_pred_energy,squared=False)\n",
    "\n",
    "        coef =  get_coeff(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var))\n",
    "        coef_cal = get_coeff(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var)*alpha**2)\n",
    "\n",
    "        ma = miscalibration_area(test_pred_energy.reshape(-1,1), np.sqrt(test_pred_energy_var).reshape(-1,1), test_energy.reshape(-1,1))\n",
    "        ma_cal = miscalibration_area(test_pred_energy.reshape(-1,1), np.sqrt(test_pred_energy_var*alpha**2).reshape(-1,1), test_energy.reshape(-1,1))\n",
    "        \n",
    "        nll = NLL(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var))\n",
    "        nll_cal = NLL(torch.tensor(test_energy), torch.tensor(test_pred_energy), torch.tensor(test_pred_energy_var)*alpha**2)\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'{identifier} {architecture} MAE: {mae:.4f} RMSE: {rmse:.4f} alpha: {alpha:.4f}, coeff: {coef:.4f}, coeff calibrated: {coef_cal:.4f}, NLL: {nll:.4f}, NLL calibrated: {nll_cal:.4f}')\n",
    "        print(f'{identifier} {architecture} MA: {ma:.4f}, MA calibrated: {ma_cal:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
